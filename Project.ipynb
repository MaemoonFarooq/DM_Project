{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DM Project\n",
        "## Muhammad Maemoon Farooq\n",
        "## 21i1680\n",
        "## DS N"
      ],
      "metadata": {
        "id": "tvok1el8CoJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "2Uu7OqcgCoJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install pmdarima"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQQS0TFRUD8R",
        "outputId": "3bffb166-a9ef-4dc8-ad26-f6fabb15f3cf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from prophet import Prophet\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.svm import SVR\n",
        "import statsmodels.api as sm\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "k9XBHTF4CoJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the csv's"
      ],
      "metadata": {
        "id": "gxaEjsfGCoJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "AEP_df = pd.read_csv('AEP_hourly.csv')\n",
        "CO2_df=pd.read_csv('Daily_atmospheric_CO2_concentration.csv')\n",
        "sp500_df=pd.read_csv('sp500_index.csv')"
      ],
      "outputs": [],
      "metadata": {
        "id": "zT_L1vIYCoJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "AEP_df.describe()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Z3H8C-SZCoJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "36c40aed-21bb-4061-f21d-8797d3cef66a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "CO2_df.describe()"
      ],
      "outputs": [],
      "metadata": {
        "id": "xbkRRBtXCoJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "5d49dda4-b56a-46ac-c8e1-712034a6b547"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sp500_df.describe()"
      ],
      "outputs": [],
      "metadata": {
        "id": "hYgRjTEjCoJv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "02d8ec45-ae32-4154-f6f2-c720591bc42e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "counts=AEP_df.isnull().sum()\n",
        "counts"
      ],
      "outputs": [],
      "metadata": {
        "id": "Abw8BwegCoJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dc63da0-b6b0-4a1f-c793-5dccbb15ae3c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "counts=sp500_df.isnull().sum()\n",
        "counts"
      ],
      "outputs": [],
      "metadata": {
        "id": "zywudnrbCoJx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffec087a-f918-4c6d-ab28-b554a8d04f40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "counts=CO2_df.isnull().sum()\n",
        "counts"
      ],
      "outputs": [],
      "metadata": {
        "id": "2nRQXt4fCoJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926e0cca-6af3-46e9-9b5c-aa906985bcdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scaling the dataset"
      ],
      "metadata": {
        "id": "n4e-bPIoCoJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def scale_time_series(df, column_name, scaler_type='min_max'):\n",
        "    values = df[column_name].values.reshape(-1, 1)\n",
        "\n",
        "    if scaler_type == 'min_max':\n",
        "        scaler = MinMaxScaler()\n",
        "    elif scaler_type == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid scaler_type. Use 'min_max' or 'standard'.\")\n",
        "\n",
        "    scaled_values = scaler.fit_transform(values)\n",
        "\n",
        "    if scaler_type == 'min_max':\n",
        "        new_column_name = f'{column_name}_min_max_scaled'\n",
        "    elif scaler_type == 'standard':\n",
        "        new_column_name = f'{column_name}_standard_scaled'\n",
        "\n",
        "    df[new_column_name] = scaled_values\n",
        "\n",
        "    return df\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "MA7j6QptCoJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "AEP_df = scale_time_series(AEP_df,column_name='AEP_MW', scaler_type='min_max')\n",
        "AEP_df = scale_time_series(AEP_df,column_name='AEP_MW', scaler_type='standard')\n",
        "AEP_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "VUTOhNgSCoJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "27ed4e3c-324c-4ac9-de1d-44d453099c40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sp500_df = scale_time_series(sp500_df,column_name='S&P500', scaler_type='min_max')\n",
        "sp500_df = scale_time_series(sp500_df,column_name='S&P500', scaler_type='standard')\n",
        "sp500_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "id7QifI8CoJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "e4ed00a6-6957-4a6f-8d5e-9491b36fb147"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "CO2_df = scale_time_series(CO2_df,column_name='day', scaler_type='min_max')\n",
        "CO2_df = scale_time_series(CO2_df,column_name='day', scaler_type='standard')\n",
        "CO2_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "TYIxNzTgCoJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "cac5e3b7-6e26-46d4-ce25-96cd9d526c74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def stationarize_time_series(df, column_name):\n",
        "    df['diff'] = df[column_name].diff()\n",
        "\n",
        "# Replacing 0 with a very small number before taking log to avoid log(0)\n",
        "    df['diff'].replace(0, np.finfo(float).eps, inplace=True)\n",
        "\n",
        "# Applying logarithmic transformation on the absolute difference\n",
        "    df['log'] = np.log(np.abs(df['diff']))\n",
        "\n",
        "# Handling initial NaN values by forward filling or other methods if suitable\n",
        "    df['log'].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    return df"
      ],
      "outputs": [],
      "metadata": {
        "id": "kyBwG78lCoJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "CO2_df = stationarize_time_series(CO2_df, column_name='day')\n",
        "AEP_df = stationarize_time_series(AEP_df, column_name='AEP_MW')\n",
        "sp500_df = stationarize_time_series(sp500_df, column_name='S&P500')"
      ],
      "outputs": [],
      "metadata": {
        "id": "UKoiysiMCoJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "sp500_df"
      ],
      "outputs": [],
      "metadata": {
        "id": "6QMG9NpzCoJ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "78dab3a8-10c9-480a-83c4-a7737f14893b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "CO2_df"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AhBnLIcfPuE9",
        "outputId": "c6513d55-cdd6-4551-c9a1-73e2396ce4d9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "AEP_df"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "snIbsmpfPupM",
        "outputId": "05916f7d-40f3-4a4c-8c67-82876ae11275"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARIMA Configuration and Tuning"
      ],
      "metadata": {
        "id": "l3ZidVpRCoJ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def adf_test(series):\n",
        "    result = adfuller(series, autolag='AIC')\n",
        "    print('ADF Statistic: %f' % result[0])\n",
        "    print('p-value: %f' % result[1])\n",
        "    print('Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print('\\t%s: %.3f' % (key, value))\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "_pWnQvYtCoJ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"SP500 ADF test\")\n",
        "adf_test(sp500_df['log'])\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "acf_vals = acf(sp500_df['log'])\n",
        "pacf_vals = pacf(sp500_df['log'])\n",
        "ax[0].stem(acf_vals, use_line_collection=True)\n",
        "ax[0].set_title('ACF Plot')\n",
        "ax[1].stem(pacf_vals, use_line_collection=True)\n",
        "ax[1].set_title('PACF Plot')\n",
        "plt.show()\n",
        "\n",
        "model = ARIMA(sp500_df['log'], order=(1, 1, 1))\n",
        "model_fit = model.fit()\n",
        "\n",
        "print(model_fit.summary())\n",
        "\n",
        "forecast = model_fit.forecast(steps=10)\n",
        "print(forecast)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0Ql9mdWoCoJ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1460c32-5bdf-44d9-dec7-50b1cb54e70c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"AEP ADF test\")\n",
        "adf_test(AEP_df['log'])\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "acf_vals = acf(AEP_df['log'])\n",
        "pacf_vals = pacf(AEP_df['log'])\n",
        "ax[0].stem(acf_vals, use_line_collection=True)\n",
        "ax[0].set_title('ACF Plot')\n",
        "ax[1].stem(pacf_vals, use_line_collection=True)\n",
        "ax[1].set_title('PACF Plot')\n",
        "plt.show()\n",
        "\n",
        "model = ARIMA(AEP_df['log'], order=(1, 1, 1))\n",
        "model_fit = model.fit()\n",
        "\n",
        "print(model_fit.summary())\n",
        "\n",
        "forecast = model_fit.forecast(steps=10)\n",
        "print(forecast)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WWz5pM_ACoJ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ac87a6f-7566-45a0-adfc-026ab3f33c2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 ADF test\")\n",
        "adf_test(CO2_df['log'])\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "acf_vals = acf(CO2_df['log'])\n",
        "pacf_vals = pacf(CO2_df['log'])\n",
        "ax[0].stem(acf_vals, use_line_collection=True)\n",
        "ax[0].set_title('ACF Plot')\n",
        "ax[1].stem(pacf_vals, use_line_collection=True)\n",
        "ax[1].set_title('PACF Plot')\n",
        "plt.show()\n",
        "\n",
        "model = ARIMA(CO2_df['log'], order=(1, 1, 1))\n",
        "model_fit = model.fit()\n",
        "\n",
        "print(model_fit.summary())\n",
        "\n",
        "forecast = model_fit.forecast(steps=10)\n",
        "print(forecast)"
      ],
      "outputs": [],
      "metadata": {
        "id": "an0gECw7CoJ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da52ecdb-523a-4dae-e686-2c48fdd14b9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN Design and Training"
      ],
      "metadata": {
        "id": "mIidBEgjJVZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Min-Max Column used to train neural network\n"
      ],
      "metadata": {
        "id": "0gCQU4ZnJY8x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train_neural_network(X_train, X_test, y_train, y_test, epochs=10, batch_size=32):\n",
        "\n",
        "    # Defining the architecture\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid activation for binary classification\n",
        "    ])\n",
        "\n",
        "    # Compiling the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
        "\n",
        "    # Evaluating the model\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(\"Test Loss:\", loss)\n",
        "    print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "    precision = precision_score(y_test, y_pred_classes)\n",
        "    recall = recall_score(y_test, y_pred_classes)\n",
        "    f1 = f1_score(y_test, y_pred_classes)\n",
        "\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "Jeo9vxeLKhox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"sp500 ANN\")\n",
        "features = sp500_df[['S&P500_min_max_scaled']]\n",
        "target = sp500_df['log']\n",
        "\n",
        "X = features.values\n",
        "y = target.values\n",
        "\n",
        "# Binarizing the target for classification\n",
        "y_binary = (y > y.mean()).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "model = train_neural_network(X_train, X_test, y_train, y_test)\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0scyQPuM7gz",
        "outputId": "89f5880c-2c28-4a84-e357-c221eacef85b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 ANN\")\n",
        "features = CO2_df[['day_min_max_scaled']]\n",
        "target = CO2_df['log']\n",
        "\n",
        "X = features.values\n",
        "y = target.values\n",
        "\n",
        "# Binarize the target for binary classification\n",
        "y_binary = (y > y.mean()).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the neural network\n",
        "model = train_neural_network(X_train, X_test, y_train, y_test)\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGMPa67dPSkZ",
        "outputId": "ea083e2b-ed64-4b6e-a326-02645496057b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"AEP ANN \")\n",
        "features = AEP_df[['AEP_MW_min_max_scaled']]  # or 'S&P500_standard_scaled'\n",
        "target = AEP_df['log']\n",
        "\n",
        "# Convert DataFrame columns to numpy arrays\n",
        "X = features.values\n",
        "y = target.values\n",
        "\n",
        "# Binarize the target for binary classification\n",
        "y_binary = (y > y.mean()).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the neural network\n",
        "model = train_neural_network(X_train, X_test, y_train, y_test)\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV8ycmS7QNGW",
        "outputId": "ed1221e0-e368-4001-ecb9-d58116c4248c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SARIMA (Seasonal ARIMA)"
      ],
      "metadata": {
        "id": "EBjXuOCxT6T_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Assuming AEP_df is already loaded and processed\n",
        "print(\"AEP Sarima Model\")\n",
        "AEP_df['Datetime'] = pd.to_datetime(AEP_df['Datetime'])\n",
        "AEP_df.set_index('Datetime', inplace=True)\n",
        "\n",
        "# Fill NaN values and prepare data\n",
        "AEP_df['AEP_MW'].fillna(method='ffill', inplace=True)\n",
        "AEP_df['AEP_MW'].fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Check for stationarity\n",
        "def test_stationarity(timeseries, window=12):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries.dropna(), autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "test_stationarity(AEP_df['AEP_MW'])\n",
        "\n",
        "# Limit the parameter space for auto_arima\n",
        "smodel = auto_arima(AEP_df['AEP_MW'], seasonal=True, m=2,\n",
        "                    trace=True, error_action='ignore', suppress_warnings=True,\n",
        "                    stepwise=True, max_order=6, max_p=2, max_q=2, max_d=1,\n",
        "                    max_P=1, max_Q=1, max_D=1)\n",
        "\n",
        "print(smodel.summary())\n",
        "\n",
        "# Fit SARIMA model using optimized parameters\n",
        "model = SARIMAX(AEP_df['AEP_MW'], order=smodel.order, seasonal_order=smodel.seasonal_order,\n",
        "                enforce_stationarity=False, enforce_invertibility=False)\n",
        "results = model.fit(disp=False)\n",
        "print(results.summary())\n",
        "\n",
        "# Forecasting and plotting results\n",
        "forecast = results.get_forecast(steps=12)\n",
        "mean_forecast = forecast.predicted_mean\n",
        "conf_int = forecast.conf_int()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(AEP_df.index, AEP_df['AEP_MW'], label='Observed')\n",
        "plt.plot(mean_forecast.index, mean_forecast, color='red', label='Forecast')\n",
        "plt.fill_between(mean_forecast.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.legend()\n",
        "plt.title('Forecast vs Actuals')\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VtcxNCQMT-LF",
        "outputId": "7e4020e5-6c9c-48d4-e519-c4951105f986"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 Sarima Model\")\n",
        "CO2_df['date'] = pd.to_datetime(CO2_df[['year', 'month', 'day']])\n",
        "CO2_df.set_index('date', inplace=True)\n",
        "\n",
        "target_column = 'cycle'\n",
        "\n",
        "CO2_df[target_column].fillna(method='ffill', inplace=True)\n",
        "CO2_df[target_column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "CO2_df[target_column].plot()\n",
        "plt.title(f'Time Series Data after Handling NaNs - {target_column}')\n",
        "plt.show()\n",
        "\n",
        "def test_stationarity(timeseries, window=12):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries.dropna(), autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "test_stationarity(CO2_df[target_column])\n",
        "\n",
        "# Determining the SARIMA parameters\n",
        "smodel = auto_arima(CO2_df[target_column], seasonal=True, m=2, trace=True, error_action='ignore', suppress_warnings=True)\n",
        "print(smodel.summary())\n",
        "\n",
        "# Fiting SARIMA model using the parameters\n",
        "model = SARIMAX(CO2_df[target_column],\n",
        "                order=smodel.order,\n",
        "                seasonal_order=smodel.seasonal_order,\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False)\n",
        "results = model.fit(disp=False)\n",
        "print(results.summary())\n",
        "\n",
        "# Forecasting values\n",
        "forecast = results.get_forecast(steps=12)\n",
        "mean_forecast = forecast.predicted_mean\n",
        "conf_int = forecast.conf_int()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(CO2_df.index, CO2_df[target_column], label='Observed')\n",
        "plt.plot(mean_forecast.index, mean_forecast, color='red', label='Forecast')\n",
        "plt.fill_between(mean_forecast.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.legend()\n",
        "plt.title(f'Forecast vs Actuals - {target_column}')\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fjW8IDOkd2qb",
        "outputId": "66795252-7054-4b98-da33-16fcca50bb8e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Ensure the 'Date' column is in datetime format and set as index\n",
        "if 'Date' in sp500_df.columns:\n",
        "    sp500_df['Date'] = pd.to_datetime(sp500_df['Date'])\n",
        "    sp500_df.set_index('Date', inplace=True)\n",
        "else:\n",
        "    raise KeyError(\"The 'Date' column is not found in the DataFrame.\")\n",
        "\n",
        "# Choose the target column for time series analysis\n",
        "target_column = 'S&P500'\n",
        "\n",
        "# Fill NaN values\n",
        "sp500_df[target_column].fillna(method='ffill', inplace=True)  # Forward fill\n",
        "sp500_df[target_column].fillna(method='bfill', inplace=True)  # Backward fill\n",
        "\n",
        "# Visualize the data to confirm no NaNs\n",
        "sp500_df[target_column].plot()\n",
        "plt.title(f'Time Series Data after Handling NaNs - {target_column}')\n",
        "plt.show()\n",
        "\n",
        "# Function to test stationarity\n",
        "def test_stationarity(timeseries, window=12):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries.dropna(), autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "# Check stationarity\n",
        "test_stationarity(sp500_df[target_column])\n",
        "\n",
        "# Determine the SARIMA parameters automatically\n",
        "smodel = auto_arima(sp500_df[target_column], seasonal=True, m=12, trace=True, error_action='ignore', suppress_warnings=True)\n",
        "print(smodel.summary())\n",
        "\n",
        "# Fit SARIMA model using the best found parameters\n",
        "model = SARIMAX(sp500_df[target_column],\n",
        "                order=smodel.order,\n",
        "                seasonal_order=smodel.seasonal_order,\n",
        "                enforce_stationarity=False,\n",
        "                enforce_invertibility=False)\n",
        "results = model.fit(disp=False)\n",
        "print(results.summary())\n",
        "\n",
        "# Forecasting future values\n",
        "forecast = results.get_forecast(steps=12)\n",
        "mean_forecast = forecast.predicted_mean\n",
        "conf_int = forecast.conf_int()\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(sp500_df.index, sp500_df[target_column], label='Observed')\n",
        "plt.plot(mean_forecast.index, mean_forecast, color='red', label='Forecast')\n",
        "plt.fill_between(mean_forecast.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.legend()\n",
        "plt.title(f'Forecast vs Actuals - {target_column}')\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3VOiVmeeeOAl",
        "outputId": "88c9f35b-a173-4858-cbd5-673c8e4f5121"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ETS"
      ],
      "metadata": {
        "id": "GMTl8F4-zKBf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def apply_ets_model(df, column, trend, seasonal, seasonal_periods):\n",
        "    train_size = int(len(df) * 0.8)\n",
        "    train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
        "\n",
        "    # Handle missing values in the data\n",
        "    train = train.dropna()\n",
        "    test = test.dropna()\n",
        "\n",
        "    ets_model = ExponentialSmoothing(train[column], trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods).fit()\n",
        "    forecast = ets_model.forecast(len(test))\n",
        "\n",
        "    mae = mean_absolute_error(test[column], forecast)\n",
        "    mse = mean_squared_error(test[column], forecast)\n",
        "    r2 = r2_score(test[column], forecast)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train.index, train[column], label='Train')\n",
        "    plt.plot(test.index, test[column], label='Test')\n",
        "    plt.plot(test.index, forecast, label='Forecast')\n",
        "    plt.legend()\n",
        "    plt.title(f'{column} - ETS Model')\n",
        "    plt.show()\n",
        "\n",
        "    return mae, mse, r2\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6uAQWtpmzLXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"AEP ETS Model:\")\n",
        "mae, mse, r2 = apply_ets_model(AEP_df, 'diff', trend='add', seasonal='add', seasonal_periods=365)\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "ZUDlT4AjztXC",
        "outputId": "0bdd4d4a-ecc4-4f72-b004-f44ddf3ff8c8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 ETS Model:\")\n",
        "mae, mse, r2 = apply_ets_model(CO2_df, 'diff', trend='add', seasonal='add', seasonal_periods=365)\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "fMQePcgBzvLQ",
        "outputId": "684514e0-a5c3-4ea3-9551-7827e6302f74"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"sp500 ETS Model:\")\n",
        "mae, mse, r2 = apply_ets_model(sp500_df, 'diff', trend='add', seasonal='add', seasonal_periods=365)\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "354ItOb3zxG_",
        "outputId": "3ede0f56-5f53-451b-c444-3ff571c5495c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prophet"
      ],
      "metadata": {
        "id": "GDehklC5z30a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "def apply_prophet_model(df, column):\n",
        "    prophet_df = df.reset_index().rename(columns={df.index.name: 'ds', column: 'y'})\n",
        "    train_size = int(len(prophet_df) * 0.8)\n",
        "    train, test = prophet_df.iloc[:train_size], prophet_df.iloc[train_size:]\n",
        "\n",
        "    prophet_model = Prophet()\n",
        "    prophet_model.fit(train)\n",
        "    forecast = prophet_model.predict(test[['ds']])\n",
        "\n",
        "    mae = mean_absolute_error(test['y'], forecast['yhat'])\n",
        "    mse = mean_squared_error(test['y'], forecast['yhat'])\n",
        "    r2 = r2_score(test['y'], forecast['yhat'])\n",
        "\n",
        "    prophet_model.plot(forecast)\n",
        "    plt.title(f'{column} - Prophet Model')\n",
        "    plt.show()\n",
        "\n",
        "    return mae, mse, r2\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "X-W-5S5jz6_i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"AEP Prophet Model:\")\n",
        "mae, mse, r2 = apply_prophet_model(AEP_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "f3nQ3leJ0ZWv",
        "outputId": "941ee920-ef99-4ee5-f2fd-b5d209fda375"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 Prophet Model:\")\n",
        "mae, mse, r2 = apply_prophet_model(CO2_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "7apUGiQ50aSW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"sp500 Prophet Model:\")\n",
        "mae, mse, r2 = apply_prophet_model(sp500_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "oObTB0z-0bMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Regression (SVR)"
      ],
      "metadata": {
        "id": "cxv8JGIz1jbj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "def apply_svr_model(df, column):\n",
        "    df = df.dropna()\n",
        "    scaler = StandardScaler()\n",
        "    scaled_data = scaler.fit_transform(df[[column]])\n",
        "    df['Scaled'] = scaled_data\n",
        "\n",
        "    train_size = int(len(df) * 0.8)\n",
        "    train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
        "\n",
        "    svr_model = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
        "    svr_model.fit(train.index.values.reshape(-1, 1), train['Scaled'])\n",
        "    forecast = scaler.inverse_transform(svr_model.predict(test.index.values.reshape(-1, 1)).reshape(-1, 1)).flatten()\n",
        "\n",
        "    mae = mean_absolute_error(test[column], forecast)\n",
        "    mse = mean_squared_error(test[column], forecast)\n",
        "    r2 = r2_score(test[column], forecast)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train.index, train[column], label='Train')\n",
        "    plt.plot(test.index, test[column], label='Test')\n",
        "    plt.plot(test.index, forecast, label='Forecast')\n",
        "    plt.legend()\n",
        "    plt.title(f'{column} - SVR Model')\n",
        "    plt.show()\n",
        "\n",
        "    return mae, mse, r2\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "JNt17PgC1lEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "print(\"AEP SVR Model:\")\n",
        "mae, mse, r2 = apply_svr_model(AEP_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpd7J1QP1q2n",
        "outputId": "effdab84-f8ae-448d-f7ff-53bf720a06ef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 SVR Model:\")\n",
        "mae, mse, r2 = apply_svr_model(CO2_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "t3e5wWwk1rGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"sp500 SVR Model:\")\n",
        "mae, mse, r2 = apply_svr_model(sp500_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "PxEVoW141rL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long Short-Term Memory (LSTM)"
      ],
      "metadata": {
        "id": "4MXCSriF14_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "def apply_lstm_model(df, column, n_steps=10):\n",
        "    df = df.dropna()\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[[column]])\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(n_steps, len(scaled_data)):\n",
        "        X.append(scaled_data[i-n_steps:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "    X, y = np.array(X), np.array(y)\n",
        "\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=(n_steps, 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
        "\n",
        "    forecast = model.predict(X_test)\n",
        "    forecast = scaler.inverse_transform(forecast)\n",
        "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "    mae = mean_absolute_error(y_test, forecast)\n",
        "    mse = mean_squared_error(y_test, forecast)\n",
        "    r2 = r2_score(y_test, forecast)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df.index[:train_size+n_steps], df[column][:train_size+n_steps], label='Train')\n",
        "    plt.plot(df.index[train_size+n_steps:], df[column][train_size+n_steps:], label='Test')\n",
        "    plt.plot(df.index[train_size+n_steps:], forecast, label='Forecast')\n",
        "    plt.legend()\n",
        "    plt.title(f'{column} - LSTM Model')\n",
        "    plt.show()\n",
        "\n",
        "    return mae, mse, r2\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "5BmIgGSv19l-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "# Apply LSTM Model to each dataset\n",
        "print(\"AEP LSTM Model:\")\n",
        "mae, mse, r2 = apply_lstm_model(AEP_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "o0FrOQS44gkz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"CO2 LSTM Model:\")\n",
        "mae, mse, r2 = apply_lstm_model(CO2_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "fFBWdBZV9g-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "print(\"sp500 LSTM Model:\")\n",
        "mae, mse, r2 = apply_lstm_model(sp500_df, 'diff')\n",
        "print(f\"MAE: {mae}, MSE: {mse}, R-squared: {r2}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "4QsS_Nr19h4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid-Model"
      ],
      "metadata": {
        "id": "OEJsRbSX_8xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "def apply_arima_model(df, column, order):\n",
        "    # Split the data into train and test sets\n",
        "    train_size = int(len(df) * 0.8)\n",
        "    train, test = df[column][:train_size], df[column][train_size:]\n",
        "\n",
        "    # Fit ARIMA model\n",
        "    model = ARIMA(train, order=order)\n",
        "    model_fit = model.fit()\n",
        "\n",
        "    # Forecast\n",
        "    forecast = model_fit.forecast(steps=len(test))\n",
        "\n",
        "    # Calculate residuals\n",
        "    residuals = test - forecast\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(test, forecast)\n",
        "    mse = mean_squared_error(test, forecast)\n",
        "    r2 = r2_score(test, forecast)\n",
        "\n",
        "    return forecast, residuals, mae, mse, r2\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "et73Yk09Axc7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Define function to train ANN on ARIMA residuals\n",
        "def train_ann_on_residuals(residuals, n_steps=10):\n",
        "    residuals = residuals.dropna().values.flatten()\n",
        "    if len(residuals) < n_steps:\n",
        "        raise ValueError(\"Not enough residuals to train the ANN model after removing NaN values.\")\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    residuals = scaler.fit_transform(residuals.reshape(-1, 1)).flatten()\n",
        "\n",
        "    X, y = [], []\n",
        "    for i in range(n_steps, len(residuals)):\n",
        "        X.append(residuals[i-n_steps:i])\n",
        "        y.append(residuals[i])\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "\n",
        "    return predictions, mae, mse, r2"
      ],
      "outputs": [],
      "metadata": {
        "id": "bb-sPngeOZLW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Define function to combine forecasts\n",
        "def combine_forecasts(arima_forecast, ann_forecast):\n",
        "    min_length = min(len(arima_forecast), len(ann_forecast))\n",
        "    arima_forecast = arima_forecast[:min_length]\n",
        "    ann_forecast = ann_forecast[:min_length]\n",
        "\n",
        "    combined_forecast = (arima_forecast + ann_forecast) / 2\n",
        "    return combined_forecast\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "3nerIuvfOZOk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Apply ARIMA model to AEP_df\n",
        "print(\"AEP ARIMA Model:\")\n",
        "aep_forecast, aep_residuals, mae, mse, r2 = apply_arima_model(AEP_df, 'AEP_MW', (5,1,0))\n",
        "print(f\"ARIMA MAE: {mae}, ARIMA MSE: {mse}, ARIMA R-squared: {r2}\")\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "_qIR2OjTOZUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Train ANN on ARIMA residuals for AEP_df\n",
        "print(\"AEP ANN on ARIMA Residuals:\")\n",
        "try:\n",
        "    aep_ann_forecast, mae, mse, r2 = train_ann_on_residuals(aep_residuals)\n",
        "    print(f\"ANN MAE: {mae}, ANN MSE: {mse}, ANN R-squared: {r2}\")\n",
        "\n",
        "    # Combine forecasts\n",
        "    aep_combined_forecast = combine_forecasts(aep_forecast, aep_ann_forecast)\n",
        "    print(\"AEP Combined Forecast:\")\n",
        "    print(aep_combined_forecast)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Nq_qtDUQOZXH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        " # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(AEP_df.index[-len(aep_combined_forecast):], AEP_df['AEP_MW'].values[-len(aep_combined_forecast):], label='Actual')\n",
        "    plt.plot(AEP_df.index[-len(aep_combined_forecast):], aep_combined_forecast, label='Combined Forecast', color='red')\n",
        "    plt.legend()\n",
        "    plt.title('AEP Combined Forecast vs Actual')\n",
        "    plt.show()\n",
        "except ValueError as e:\n",
        "    print(e)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lyi95nneOwgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "JojEvrP-PLGD"
      }
    }
  ],
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}